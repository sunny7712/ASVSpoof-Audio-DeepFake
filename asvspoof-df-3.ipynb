{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":3842332,"sourceType":"datasetVersion","datasetId":2286778}],"dockerImageVersionId":30673,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from transformers import Wav2Vec2FeatureExtractor, Wav2Vec2Processor, Wav2Vec2Config, Wav2Vec2Model\nfrom transformers.modeling_outputs import TokenClassifierOutput\nfrom datasets import Dataset\nimport numpy as np\nimport torch\nfrom torch import nn\nimport torchaudio\nimport os\nfrom tqdm.auto import tqdm\nimport matplotlib.pyplot as plt\nimport math\nimport timm\nfrom sklearn.metrics import roc_auc_score, roc_curve, RocCurveDisplay, f1_score, classification_report, ConfusionMatrixDisplay","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-04-24T11:21:40.059465Z","iopub.execute_input":"2024-04-24T11:21:40.059828Z","iopub.status.idle":"2024-04-24T11:21:52.004854Z","shell.execute_reply.started":"2024-04-24T11:21:40.059798Z","shell.execute_reply":"2024-04-24T11:21:52.003791Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_audio_files_path = '/kaggle/input/asvpoof-2019-dataset/LA/LA/ASVspoof2019_LA_train/flac'\ntrain_labels_path = '/kaggle/input/asvpoof-2019-dataset/LA/LA/ASVspoof2019_LA_cm_protocols/ASVspoof2019.LA.cm.train.trn.txt'\nlen(os.listdir(train_audio_files_path))","metadata":{"execution":{"iopub.status.busy":"2024-04-24T11:21:52.006700Z","iopub.execute_input":"2024-04-24T11:21:52.007232Z","iopub.status.idle":"2024-04-24T11:21:53.959309Z","shell.execute_reply.started":"2024-04-24T11:21:52.007206Z","shell.execute_reply":"2024-04-24T11:21:53.958294Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def readtxtfile(path):\n    with open(path, 'r') as file:\n        text = file.read().splitlines()\n        return text\n    \ndef getlabels(path):\n    text = readtxtfile(path)\n    filename2label = {}\n    for item in tqdm(text):\n        key = item.split(' ')[1]\n        value = item.split(' ')[-1]\n        filename2label[key] = value\n        \n    return filename2label","metadata":{"execution":{"iopub.status.busy":"2024-04-24T11:21:53.960363Z","iopub.execute_input":"2024-04-24T11:21:53.960633Z","iopub.status.idle":"2024-04-24T11:21:53.966817Z","shell.execute_reply.started":"2024-04-24T11:21:53.960609Z","shell.execute_reply":"2024-04-24T11:21:53.965889Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"filename2label = getlabels(train_labels_path)","metadata":{"execution":{"iopub.status.busy":"2024-04-24T11:21:53.968921Z","iopub.execute_input":"2024-04-24T11:21:53.969598Z","iopub.status.idle":"2024-04-24T11:21:54.053471Z","shell.execute_reply.started":"2024-04-24T11:21:53.969562Z","shell.execute_reply":"2024-04-24T11:21:54.052630Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"val_audio_files_path = '/kaggle/input/asvpoof-2019-dataset/LA/LA/ASVspoof2019_LA_dev/flac'\nval_labels_path = '/kaggle/input/asvpoof-2019-dataset/LA/LA/ASVspoof2019_LA_cm_protocols/ASVspoof2019.LA.cm.dev.trl.txt'\nval_filename2label = getlabels(val_labels_path)","metadata":{"execution":{"iopub.status.busy":"2024-04-24T11:21:54.054691Z","iopub.execute_input":"2024-04-24T11:21:54.054965Z","iopub.status.idle":"2024-04-24T11:21:54.131433Z","shell.execute_reply.started":"2024-04-24T11:21:54.054941Z","shell.execute_reply":"2024-04-24T11:21:54.130476Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_audio_files_path = '/kaggle/input/asvpoof-2019-dataset/LA/LA/ASVspoof2019_LA_eval/flac'\ntest_labels_path = '/kaggle/input/asvpoof-2019-dataset/LA/LA/ASVspoof2019_LA_cm_protocols/ASVspoof2019.LA.cm.eval.trl.txt'\ntest_filename2label = getlabels(test_labels_path)","metadata":{"execution":{"iopub.status.busy":"2024-04-24T11:21:54.132653Z","iopub.execute_input":"2024-04-24T11:21:54.132953Z","iopub.status.idle":"2024-04-24T11:21:54.302017Z","shell.execute_reply.started":"2024-04-24T11:21:54.132927Z","shell.execute_reply":"2024-04-24T11:21:54.301045Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class ASVSpoof(torch.utils.data.Dataset):\n    def __init__(self, audio_dir_path, num_samples, filename2label, transforms):\n        super().__init__()\n        self.audio_dir_path = audio_dir_path\n        self.num_samples = num_samples\n        self.audio_file_names = self.get_audio_file_names(filename2label)\n        self.labels, self.label2id, self.id2label = self.get_labels(filename2label)\n        self.transforms = transforms\n        \n    def __getitem__(self, index):\n        signal, sr = torchaudio.load(os.path.join(self.audio_dir_path, self.audio_file_names[index]))\n#         print(signal.shape)\n        signal = self.mix_down_if_necessary(signal)\n        signal = self.cut_if_necessary(signal)\n#         print(signal.shape)\n        signal = self.right_pad_if_necessary(signal)\n#         print(signal.shape)\n#         signal = self.transforms(signal)\n#         print(signal.shape)\n        label = (self.labels[index])\n        return signal, label\n    \n    def __len__(self):\n        return len(self.labels)\n    \n    def get_audio_file_names(self, filename2label):\n        audio_file_names = list(filename2label.keys())\n        audio_file_names = [name + '.flac' for name in audio_file_names] # adding extension\n        return audio_file_names\n    \n    def get_labels(self, filename2label):\n        labels = list(filename2label.values())\n        id2label = {idx : label for idx, label in  enumerate(list(set(labels)))}\n        label2id = {label : idx for idx, label in  enumerate(list(set(labels)))}\n        labels = [label2id[label] for label in labels]\n        return labels, label2id, id2label\n    \n    def mix_down_if_necessary(self, signal): #converting from stereo to mono\n        if signal.shape[0] > 1: \n            signal = torch.mean(signal, dim = 0, keepdims = True)\n        return signal\n    \n    def cut_if_necessary(self, signal):\n        if signal.shape[1] > self.num_samples:\n            signal = signal[:, :num_samples]\n        return signal\n    \n    def right_pad_if_necessary(self, signal):\n        length = signal.shape[1]\n        if self.num_samples > length:\n            pad_last_dim = (0, num_samples - length)\n            signal = torch.nn.functional.pad(signal, pad_last_dim)\n        return signal","metadata":{"execution":{"iopub.status.busy":"2024-04-24T11:21:57.749183Z","iopub.execute_input":"2024-04-24T11:21:57.749991Z","iopub.status.idle":"2024-04-24T11:21:57.765352Z","shell.execute_reply.started":"2024-04-24T11:21:57.749961Z","shell.execute_reply":"2024-04-24T11:21:57.764490Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"num_samples = 4 * 16000 # IMPORTANT!!\ntrain_dataset = ASVSpoof(train_audio_files_path, num_samples, filename2label, None)\nval_dataset = ASVSpoof(val_audio_files_path, num_samples, val_filename2label, None)\ntest_dataset = ASVSpoof(test_audio_files_path, num_samples, test_filename2label, None)","metadata":{"execution":{"iopub.status.busy":"2024-04-24T11:21:59.037259Z","iopub.execute_input":"2024-04-24T11:21:59.037610Z","iopub.status.idle":"2024-04-24T11:21:59.082268Z","shell.execute_reply.started":"2024-04-24T11:21:59.037580Z","shell.execute_reply":"2024-04-24T11:21:59.081239Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_dataset[28][0].shape","metadata":{"execution":{"iopub.status.busy":"2024-04-24T11:22:02.039177Z","iopub.execute_input":"2024-04-24T11:22:02.040000Z","iopub.status.idle":"2024-04-24T11:22:02.152335Z","shell.execute_reply.started":"2024-04-24T11:22:02.039963Z","shell.execute_reply":"2024-04-24T11:22:02.151444Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_loader = torch.utils.data.DataLoader(train_dataset, shuffle = True, batch_size = 8)\nval_loader = torch.utils.data.DataLoader(val_dataset, shuffle = True, batch_size = 8)\ntest_loader = torch.utils.data.DataLoader(test_dataset, shuffle = True, batch_size = 8)\nt_steps = len(train_loader)\nv_steps = len(val_loader)\nts_steps = len(test_loader)","metadata":{"execution":{"iopub.status.busy":"2024-04-24T13:02:48.672948Z","iopub.execute_input":"2024-04-24T13:02:48.673730Z","iopub.status.idle":"2024-04-24T13:02:48.679619Z","shell.execute_reply.started":"2024-04-24T13:02:48.673695Z","shell.execute_reply":"2024-04-24T13:02:48.678557Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"torch.autograd.set_detect_anomaly(True)","metadata":{"execution":{"iopub.status.busy":"2024-04-24T13:02:49.051365Z","iopub.execute_input":"2024-04-24T13:02:49.051995Z","iopub.status.idle":"2024-04-24T13:02:49.057534Z","shell.execute_reply.started":"2024-04-24T13:02:49.051968Z","shell.execute_reply":"2024-04-24T13:02:49.056587Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define the model\nclass CustomWav2Vec2ForClassification(nn.Module):\n    def __init__(self, checkpoint):\n        super(CustomWav2Vec2ForClassification, self).__init__()\n        config = Wav2Vec2Config.from_pretrained(checkpoint)\n        self.feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(checkpoint)\n        print(self.feature_extractor)\n        self.wav2vec2 = Wav2Vec2Model.from_pretrained(checkpoint, config=config)\n        self.blstm = nn.LSTM(config.hidden_size, config.hidden_size // 2, bidirectional = True, num_layers = 2, batch_first = True)\n        self.relu1 = nn.ReLU()\n        self.dropout1 = nn.Dropout(0.1)\n        self.pool = nn.AdaptiveAvgPool1d(128)\n        self.linear = nn.Linear(199 * 128, 1)\n        self.sigmoid = nn.Sigmoid()\n\n    def forward(self, input_ids, attention_mask):\n\n        input_features = self.feature_extractor(input_ids, return_tensors=\"pt\", sampling_rate = 16000)\n        \n        ff = input_features.input_values\n        ff = ff.squeeze(0).to(device)\n\n        attention_mask = attention_mask.to(device)\n        # Extract features from input audio\n        features = self.wav2vec2(ff, attention_mask=attention_mask, output_hidden_states = True).last_hidden_state\n        \n        # Pooling the last hidden states\n#         pooled_output = torch.mean(features, dim=1)  # Average pooling\n#         print(features.shape) # (batch_size, 199, 1024)\n        # Classification\n        res = features\n        x,(h, c) = self.blstm(features) # (batch_size, 199, 1024)\n#         print(x.shape)\n        x = x + res # (batch_size, 199, 1024)\n#         x = self.relu1(x)\n#         x = self.dropout1(x)\n        x = self.pool(x) #(batch_size, 199, 128)\n        x = x.view(x.shape[0], -1) #(batch_size, 199 * 128)\n#         print(x.shape)\n        x = self.linear(x)\n        x = self.sigmoid(x)\n        return x","metadata":{"execution":{"iopub.status.busy":"2024-04-24T13:02:50.108396Z","iopub.execute_input":"2024-04-24T13:02:50.109125Z","iopub.status.idle":"2024-04-24T13:02:50.119760Z","shell.execute_reply.started":"2024-04-24T13:02:50.109083Z","shell.execute_reply":"2024-04-24T13:02:50.118820Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"checkpoint = 'facebook/wav2vec2-large-xlsr-53'\nmodel = CustomWav2Vec2ForClassification(checkpoint)\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\n\n# Define optimizer and loss function\noptimizer = torch.optim.AdamW(model.parameters(), lr=1e-6)\ncriterion = nn.CrossEntropyLoss()","metadata":{"execution":{"iopub.status.busy":"2024-04-24T13:02:51.200275Z","iopub.execute_input":"2024-04-24T13:02:51.200651Z","iopub.status.idle":"2024-04-24T13:02:52.296592Z","shell.execute_reply.started":"2024-04-24T13:02:51.200624Z","shell.execute_reply":"2024-04-24T13:02:52.295805Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# for i,(name, param) in enumerate(list(model.named_parameters())):\n#     if(name[:4] == 'wav2'):\n#         param.requires_grad = False","metadata":{"execution":{"iopub.status.busy":"2024-04-24T13:02:53.238469Z","iopub.execute_input":"2024-04-24T13:02:53.239222Z","iopub.status.idle":"2024-04-24T13:02:53.243087Z","shell.execute_reply.started":"2024-04-24T13:02:53.239188Z","shell.execute_reply":"2024-04-24T13:02:53.242091Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import gc\nx = 100\nwhile(x != 0):\n    x = gc.collect()\n    torch.cuda.empty_cache()\nx","metadata":{"execution":{"iopub.status.busy":"2024-04-24T13:02:54.163782Z","iopub.execute_input":"2024-04-24T13:02:54.164143Z","iopub.status.idle":"2024-04-24T13:02:55.287082Z","shell.execute_reply.started":"2024-04-24T13:02:54.164104Z","shell.execute_reply":"2024-04-24T13:02:55.285969Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def EER(labels, outputs):\n    fpr, tpr, threshold = roc_curve(labels, outputs, pos_label=1)\n    fnr = 1 - tpr\n    eer_threshold = threshold[np.nanargmin(np.absolute((fnr - fpr)))]\n    eer_threshold\n    eer = fpr[np.nanargmin(np.absolute((fnr - fpr)))]\n    return eer","metadata":{"execution":{"iopub.status.busy":"2024-04-24T13:02:56.313090Z","iopub.execute_input":"2024-04-24T13:02:56.314017Z","iopub.status.idle":"2024-04-24T13:02:56.319408Z","shell.execute_reply.started":"2024-04-24T13:02:56.313980Z","shell.execute_reply":"2024-04-24T13:02:56.318478Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Training loop\nnum_epochs = 8\ntrain_losses = []\nval_losses = []\ntorch.cuda.empty_cache()\nfor epoch in range(num_epochs):\n    y_true = []\n    y_pred = []\n    train_loss = 0.0\n    loop = tqdm(enumerate(train_loader), total = len(train_loader))\n    for batch_idx, (input_ids, labels) in loop:\n        loop.set_description(f'Epoch {epoch + 1} / {num_epochs} ')\n#         forward pass\n        model.train()\n        torch.cuda.empty_cache()\n        input_ids = input_ids.to(device)\n        input_ids = input_ids.squeeze(1)\n        labels = labels.to(device)\n        labels = labels.to(device).reshape(-1, 1)\n        labels = labels.type(torch.cuda.FloatTensor)\n        attention_mask = torch.ones(input_ids.shape, dtype=torch.long).to(device)\n        \n        optimizer.zero_grad()\n        \n        outputs = model(input_ids, attention_mask)\n        y_true.append(labels.detach().cpu().numpy())\n        y_pred.append(outputs.detach().cpu().numpy())\n        \n        loss = criterion(outputs, labels)\n        train_loss += loss.item()\n#         backward pass\n        loss.backward()\n        optimizer.step()\n        \n        loop.set_postfix(Training_loss = loss.item())\n    \n    y_true = np.concatenate(y_true)\n    y_pred = np.concatenate(y_pred)\n    train_eer = EER(y_true, y_pred)\n        \n#   validation every epoch\n    y_true = []\n    y_pred = []\n    model.eval()\n    val_loss = 0.0\n    with torch.no_grad():\n        val_loop = tqdm(enumerate(val_loader), total = len(val_loader))\n        for val_batch_idx, (val_input_ids, val_labels) in val_loop:\n            torch.cuda.empty_cache()\n            val_input_ids = val_input_ids.to(device)\n            val_input_ids = val_input_ids.squeeze(1)\n            val_labels = val_labels.to(device)\n            val_labels = val_labels.to(device).reshape(-1, 1)\n            val_labels = val_labels.type(torch.cuda.FloatTensor) #use torch.FloatTensor if on cpu\n            attention_mask = torch.ones(val_input_ids.shape, dtype=torch.long).to(device)\n        \n        \n            val_outputs = model(val_input_ids, attention_mask)\n            y_true.append(val_labels.detach().cpu().numpy())\n            y_pred.append(val_outputs.detach().cpu().numpy())\n            curr_val_loss = criterion(val_outputs, val_labels)\n            val_loss += curr_val_loss.item()\n            val_loop.set_postfix(validation_loss = curr_val_loss.item())\n            \n    train_loss_after_epoch = train_loss / t_steps\n    val_loss_after_epoch = val_loss / v_steps\n    train_losses.append(train_loss_after_epoch)\n    val_losses.append(val_loss_after_epoch)\n    y_true = np.concatenate(y_true)\n    y_pred = np.concatenate(y_pred)\n    val_eer = EER(y_true, y_pred)\n    print(f'Epoch : {epoch + 1} Training loss : {train_loss_after_epoch} Train EER : {train_eer} Validation loss : {val_loss_after_epoch}  Val EER : {val_eer}')","metadata":{"execution":{"iopub.status.busy":"2024-04-24T13:02:57.884667Z","iopub.execute_input":"2024-04-24T13:02:57.885914Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"new_outputs = []\nnew_labels = []\nmodel.eval()\ntest_loss = 0.0\nwith torch.no_grad():\n    test_loop = tqdm(enumerate(test_loader), total = len(test_loader))\n    for test_batch_idx, (test_input_ids, test_labels) in test_loop:\n        torch.cuda.empty_cache()\n        test_input_ids = test_input_ids.to(device)\n        test_input_ids = test_input_ids.squeeze(1)\n        test_labels = test_labels.to(device)\n        test_labels = test_labels.to(device).reshape(-1, 1)\n        test_labels = test_labels.type(torch.cuda.FloatTensor) #use torch.FloatTensor if on cpu\n        attention_mask = torch.ones(test_input_ids.shape, dtype=torch.long).to(device)\n\n\n        test_outputs = model(test_input_ids, attention_mask)\n        new_outputs.append(test_outputs.cpu().numpy())\n        new_labels.append(test_labels.cpu().numpy())\n        curr_test_loss = criterion(test_outputs, test_labels)\n        test_loss += curr_test_loss.item()\n        test_loop.set_postfix(test_loss = curr_test_loss.item())","metadata":{"execution":{"iopub.status.busy":"2024-04-24T12:20:26.068203Z","iopub.execute_input":"2024-04-24T12:20:26.068566Z","iopub.status.idle":"2024-04-24T12:20:26.588063Z","shell.execute_reply.started":"2024-04-24T12:20:26.068538Z","shell.execute_reply":"2024-04-24T12:20:26.586862Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"new_labels = np.concatenate(new_labels)\nnew_outputs = np.concatenate(new_outputs)\nprint(new_labels.shape, new_outputs.shape)","metadata":{"execution":{"iopub.status.busy":"2024-04-09T13:44:17.180269Z","iopub.execute_input":"2024-04-09T13:44:17.180591Z","iopub.status.idle":"2024-04-09T13:44:17.189500Z","shell.execute_reply.started":"2024-04-09T13:44:17.180566Z","shell.execute_reply":"2024-04-09T13:44:17.188505Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fpr, tpr, threshold = roc_curve(new_labels, new_outputs, pos_label=1)\nfnr = 1 - tpr\neer_threshold = threshold[np.nanargmin(np.absolute((fnr - fpr)))]\neer_threshold","metadata":{"execution":{"iopub.status.busy":"2024-04-09T13:44:17.190741Z","iopub.execute_input":"2024-04-09T13:44:17.191089Z","iopub.status.idle":"2024-04-09T13:44:17.217317Z","shell.execute_reply.started":"2024-04-09T13:44:17.191057Z","shell.execute_reply":"2024-04-09T13:44:17.216313Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"EER = fpr[np.nanargmin(np.absolute((fnr - fpr)))]\nEER","metadata":{"execution":{"iopub.status.busy":"2024-04-09T13:44:17.219335Z","iopub.execute_input":"2024-04-09T13:44:17.219628Z","iopub.status.idle":"2024-04-09T13:44:17.226666Z","shell.execute_reply.started":"2024-04-09T13:44:17.219605Z","shell.execute_reply":"2024-04-09T13:44:17.225582Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"torch.save(model.state_dict(), 'wav2vec2.pt')","metadata":{},"execution_count":null,"outputs":[]}]}